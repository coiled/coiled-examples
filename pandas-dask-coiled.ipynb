{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas to Dask to Coiled\n",
    "\n",
    "We scale a simple ETL workflow from a single thread to the cloud\n",
    "\n",
    "1.  Pandas on a sample of data\n",
    "2.  Pandas on a complete file\n",
    "3.  Dask + Pandas on a set of files in parallel\n",
    "4.  Coiled + Dask + Pandas to run on the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas: Convert CSV to Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data from Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2019-{01..12}.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/mrocklin/data/nyctaxi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate data locally with Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"~/data/nyctaxi/yellow_tripdata_2019-01.csv\", \n",
    "    nrows=10000,\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ~/data/nyctaxi/yellow_tripdata_2019-01.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Massage data types \n",
    "\n",
    "Before we convert to parquet format, let's clean up our types a little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"~/data/nyctaxi/yellow_tripdata_2019-01.csv\", \n",
    "    parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"],\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({\n",
    "    \"VendorID\": \"uint8\",\n",
    "    \"passenger_count\": \"uint8\",\n",
    "    \"RatecodeID\": \"uint8\",\n",
    "    \"store_and_fwd_flag\": \"category\",\n",
    "    \"PULocationID\": \"uint16\",\n",
    "    \"DOLocationID\": \"uint16\",    \n",
    "})\n",
    "\n",
    "df[\"tip_amount\"] = df.total_amount / df.tip_amount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.to_parquet(\"~/data/nyctaxi/yellow_tripdata_2019-01.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_parquet(\"~/data/nyctaxi/yellow_tripdata_2019-01.parq\", columns=\"passenger_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operate on many files in a for loop?\n",
    "\n",
    "We could do this, but it's unpleasant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in glob(\"~/data/nyctaxi/yellow_tripdata_2019-*.parq\"):\n",
    "    df = pd.read_csv(filename)\n",
    "    ...\n",
    "    df.to_parquet(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Dask locally to process the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster, Client\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv(\n",
    "    \"~/data/nyctaxi/yellow_tripdata_2019-*.csv\", \n",
    "    parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"],\n",
    "    dtype={'RatecodeID': 'float64',\n",
    "       'VendorID': 'float64',\n",
    "       'passenger_count': 'float64',\n",
    "       'payment_type': 'float64'}\n",
    "\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({\n",
    "    \"VendorID\": \"UInt8\",\n",
    "    \"passenger_count\": \"UInt8\",\n",
    "    \"RatecodeID\": \"UInt8\",\n",
    "    \"store_and_fwd_flag\": \"category\",\n",
    "    \"PULocationID\": \"UInt16\",\n",
    "    \"DOLocationID\": \"UInt16\",    \n",
    "})\n",
    "\n",
    "df[\"tip_amount\"] = df.total_amount / df.tip_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.to_parquet(\"~/data/nyctaxi/yellow_tripdata_2019.parq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We could target the data on S3 directly\n",
    "\n",
    "This data lives on Amazon S3, in a bucket by the NYC Taxi and Livery Commision, `nyc-tlc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "s3 = s3fs.S3FileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.ls(\"nyc-tlc/trip data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with s3.open(\"nyc-tlc/trip data/yellow_tripdata_2019-12.csv\") as f:\n",
    "    print(f.read(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work directly from the cloud with Coiled "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create software environment\n",
    "\n",
    "We'll need Dask, Arrow, and s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coiled.create_software_environment(\n",
    "    name=\"parquet\", \n",
    "    conda=[\"dask\", \"pyarrow\", \"s3fs\"],\n",
    ")\n",
    "\n",
    "coiled.create_cluster_configuration(\n",
    "    name=\"parquet\", \n",
    "    software=\"parquet\", \n",
    "    worker_cpu=4, \n",
    "    worker_memory=\"16 GiB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Cluster\n",
    "\n",
    "It will take about a minute to provision our resources on the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = CoiledCluster(n_workers=10, configuration=\"parquet\")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.read_csv(\n",
    "    \"s3://nyc-tlc/trip data/yellow_tripdata_2019-*.csv\", \n",
    "    parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"],\n",
    "    dtype={\n",
    "        'RatecodeID': 'float64',\n",
    "       'VendorID': 'float64',\n",
    "       'passenger_count': 'float64',\n",
    "       'payment_type': 'float64'\n",
    "    },\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({\n",
    "    \"VendorID\": \"UInt8\",\n",
    "    \"passenger_count\": \"UInt8\",\n",
    "    \"RatecodeID\": \"UInt8\",\n",
    "    \"store_and_fwd_flag\": \"category\",\n",
    "    \"PULocationID\": \"UInt16\",\n",
    "    \"DOLocationID\": \"UInt16\",    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.to_parquet(\"s3://coiled-data/nyctaxi-2019.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
