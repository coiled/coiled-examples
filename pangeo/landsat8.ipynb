{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landsat-8 with Coiled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dask cluster with Coiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(n_workers=10, configuration=\"coiled-examples/pangeo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ☝️ Don’t forget to click the \"Dashboard\" link above to view the cluster dashboard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Landsat-8 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from satsearch import Search\n",
    "import intake\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "def load_landsat8_data():\n",
    "    # NOTE this STAC API endpoint does not currently search the entire catalog\n",
    "    bbox = (-124.71, 45.47, -116.78, 48.93) #(west, south, east, north) \n",
    "\n",
    "    time_range = '2019-01-01/2020-10-01'\n",
    "\n",
    "    # STAC metadata properties\n",
    "    properties = ['eo:row=027', 'eo:column=047', 'landsat:tier=T1'] \n",
    "    results = Search.search(collection='landsat-8-l1', \n",
    "                            bbox=bbox,\n",
    "                            datetime=time_range,\n",
    "                            property=properties,\n",
    "                            sort=['<datetime'])\n",
    "\n",
    "    items = results.items()\n",
    "    items.save('subset.geojson')\n",
    "    \n",
    "    gf = gpd.read_file('subset.geojson')\n",
    "    band_info = pd.DataFrame(ast.literal_eval(gf.iloc[0]['eo:bands']))\n",
    "    bands = band_info.query('gsd == 30').common_name.to_list()\n",
    "    \n",
    "    @dask.delayed\n",
    "    def stacitem_to_dataset(item):\n",
    "        stacked = catalog[item.id].stack_bands(bands)\n",
    "        da = stacked(chunks=dict(band=1, x=8000, y=2048)).to_dask()\n",
    "        da['band'] = bands # use common names\n",
    "        da = da.expand_dims(time=[pd.to_datetime(item.datetime)])\n",
    "        ds = da.to_dataset(dim='band')\n",
    "        return ds\n",
    "\n",
    "    catalog = intake.open_stac_item_collection(items)\n",
    "    lazy_datasets = []\n",
    "    for i, item in gf.iterrows():\n",
    "        ds = stacitem_to_dataset(item)\n",
    "        lazy_datasets.append(ds)\n",
    "    datasets = dask.compute(*lazy_datasets)\n",
    "    \n",
    "    return xr.concat(datasets, dim='time')\n",
    "\n",
    "DS = load_landsat8_data()\n",
    "print(f\"Dataset size: {DS.nbytes / 1e9} [Gb]\")\n",
    "DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed NDVI computation\n",
    "\n",
    "Now that we have our `xarray.DataSet` which contains the Landsat-8 dataset, we'll calculate the NDVI with all our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDVI = (DS['nir'] - DS['red']) / (DS['nir'] + DS['red'])\n",
    "NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDVI.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then calculate and plot the average NDVI over some spatial selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDVI.sel(x=slice(4.5e5, 5.0e5), y=slice(5.25e6, 5.2e6)).mean(dim=['time']).plot.imshow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "For more examples on how Dask can scale geoscience workloads, we recommend looking at the [Pangeo](http://pangeo.io/) community's [example gallery](http://gallery.pangeo.io/) which contain interactive examples that analyze large-scale datasets using Dask, [Xarray](http://xarray.pydata.org/en/stable/), and other open source PyData libraries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
